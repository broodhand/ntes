# -*- coding: utf-8 -*-"""Created on Wed Mar 23 12:37:57 2016@author: Zhao Cheng__version__ = '0.8.0'Asynchronous getting the restful api datas"""import logging; logging.basicConfig(level=logging.INFO)import asyncioimport aiohttpimport uuidimport timefrom collections import Iteratornamespace = uuid.uuid1()_count = int()_success = int()_failure = int()_async = 1def _get_uuid(namespace_myuuid, url):    "To get the now_stand and uuid"    now_stamp = int(time.time())    now_stand = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(now_stamp))    uuid_name = '%s/%s' % (url, now_stamp)    myuuid = uuid.uuid3(namespace_myuuid, uuid_name).hex    return now_stand, myuuidasync def _fetch(session, loop, url, timeout=1, res_type='text', encoding='utf-8'):    "To asynchronous getting a url's data"    choice = dict(text=lambda self: self.text(encoding=encoding),                  json=lambda self: self.json(),                  bytes=lambda self: self.read())    global namespace, _async    _async += 1    logging.debug('session number: %s' % str(_async))    now_stand, myuuid = _get_uuid(namespace, url)    result = dict(url=url, time=now_stand, uuid=myuuid)    try:        with aiohttp.Timeout(timeout, loop=loop):            async with session.get(url) as res:                content = await choice[res_type](res)                status = res.status                result.update(dict(status=status, content=content))                logging.debug('_fetch get content: %s' % result)    except asyncio.TimeoutError as e:        result.update(dict(status=408, content=None))        logging.warning('<aiohttpAPI.client._fetch> time out: %s' % e)    except aiohttp.errors.ClientOSError as e:        result.update(dict(status=500, content=None))        logging.warning('<aiohttpAPI.client._fetch> network error %s' % e)    except Exception as e:        result.update(dict(status=500, content=None))        logging.warning('<aiohttpAPI.client._fetch> unknown error %s' % e)    finally:        _async -= 1        return resultasync def _retry(url, retry_times=3, callback_function=None, filter_function=None, **kwargs):    "When encountered errors,Retry to get data"    global _success, _failure, _count, _async    loop = kwargs.pop('loop')    sem = kwargs.pop('sem')    async with aiohttp.ClientSession(loop=loop) as session:        with (await sem):            result = await _fetch(session, loop, url, **kwargs)            retry = 0            while retry <= retry_times and result['status'] != 200:                result = await _fetch(session, loop, url, **kwargs)                retry += 1                result['retry'] = retry    _count += 1    if result['status'] == 200:        _success += 1    else:        _failure += 1    logging.info(        '<aiohttpAPI> Count %d/Success %d/Failure %d/Asyncio session %d' % (_count, _success, _failure, _async))    if callback_function:        callback_function(result)    if filter_function:        filter_result = filter_function(result)        if filter_result:            return filter_result    else:        return resultdef get_urls(urls, semaphore=20, proc_function=None, **kwargs):    """    To asynchronous getting datas from restful api.    :param urls: Input a url list for getting datas.    :param semaphore: The max sessions to connect servers at the same time.    :param proc_function: to process the result's content    :param :            retry_times=3: retry times            semaphore=20: The max sessions to connect servers at the same time.            timeout=1: the timeout of sessions            res_type='text': type of data. 'text':str 'bytes':bytes 'json':auto using json encoder            encode='utf-8': the data's str code            callback_function=None: logging.info(self): to process result in time            filter_function=None to filter result's content            proc_function=None to process the result's content    :return: the result of the urls' content    """    if not isinstance(urls, (tuple, list, Iterator)):        raise TypeError('Must be tuple, list, Iterator')    global _count, _success, _failure    loop = asyncio.get_event_loop()    sem = asyncio.Semaphore(semaphore, loop=loop)    kwargs['loop'] = loop    kwargs['sem'] = sem    result_list = list()    tasks = list()    for url in urls:        tasks.append(asyncio.ensure_future(_retry(url, **kwargs)))    start_time = int(time.time())    loop.run_until_complete(asyncio.wait(tasks))    logging.info('<aiohttpAPI> Success: use %s s/count: %s/success: %s/failure: %s' % (        str(int(time.time()) - start_time), _count, _success, _failure))    for task in tasks:        if task.result():            result_list.append(task.result())    tasks.clear()    _count = int()    _success = int()    _failure = int()    if proc_function:        return proc_function(result_list)    else:        return result_listdef get_url(*url, **kwargs):    return get_urls(url, **kwargs)